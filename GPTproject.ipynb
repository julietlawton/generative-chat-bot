{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os  # Operating System module\n",
    "import chardet  # Character encoding detection library\n",
    "import re  # Regular expression library\n",
    "import ast  # Abstract Syntax Trees module for parsing Python code\n",
    "import pickle  # Module for serializing and deserializing Python objects\n",
    "import itertools  # Module for efficient looping and iteration\n",
    "import pandas as pd  # Pandas library for data manipulation and analysis\n",
    "import numpy as np  # NumPy library for numerical operations\n",
    "import matplotlib.pyplot as plt  # Matplotlib library for data visualization\n",
    "from transformers import GPT2Tokenizer  # GPT-2 Tokenizer from Hugging Face's Transformers library\n",
    "from pandarallel import pandarallel  # Parallel processing with pandas\n",
    "\n",
    "# Set display option for Pandas to show entire column width\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Initialize parallel processing with pandas\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename for the dataset\n",
    "DATASET_FILENAME = \"movie_conversations.pkl\"\n",
    "\n",
    "# Define the maximum token length for the dataset\n",
    "MAX_TOKEN_LENGTH = 1024\n",
    "\n",
    "# Flag to track whether the dataset has been generated or not\n",
    "dataset_generated = False\n",
    "\n",
    "# Check if the dataset file already exists\n",
    "if os.path.exists(DATASET_FILENAME):\n",
    "    # If the file exists, set the dataset_generated flag to True\n",
    "    dataset_generated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Detect the encoding of the movie lines file\n",
    "    with open(cwd + \"/movie_lines/movie_lines.txt\", \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        movie_lines_encoding = result[\"encoding\"]\n",
    "\n",
    "    # Detect the encoding of the movie conversations file\n",
    "    with open(cwd + \"/movie_lines/movie_conversations.txt\", \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        movie_conversations_encoding = result[\"encoding\"]\n",
    "\n",
    "    # Print the detected encodings\n",
    "    print(movie_lines_encoding)\n",
    "    print(movie_conversations_encoding)\n",
    "\n",
    "    # Note: The lines below are commented out. If needed, you can manually set encodings.\n",
    "    # movie_lines_encoding = \"Windows-1252\"\n",
    "    # movie_conversations_encoding = \"ascii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Collect individual movie lines\n",
    "    with open(cwd + \"/movie_lines/movie_lines.txt\", \"r\", encoding=movie_lines_encoding) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split the content into individual lines\n",
    "    lines = content.split(\"\\n\")\n",
    "\n",
    "    # Print the first 5 lines to verify content\n",
    "    print(lines[:5])\n",
    "\n",
    "    # Remove the last element of lines because it's an empty string\n",
    "    lines = lines[:-1]\n",
    "\n",
    "    # Print the length of the lines to verify correctness\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Initialize containers for values to put in the dataframe\n",
    "    line_numbers_dict = {}\n",
    "\n",
    "    # Iterate through each line in the lines list\n",
    "    for line in lines:\n",
    "        # Split the line on whitespace\n",
    "        split = line.split(\" \")\n",
    "        # Extract line number and character ID\n",
    "        line_number = split[0]\n",
    "        character_id = split[2]\n",
    "\n",
    "        # Extract the text after the last \"+\" character using regular expression\n",
    "        # This is a way to split the line on the last \"+\" in case there are other \"+\" characters in the text\n",
    "        l = re.split(r'\\+\\s+(?=[^+]*$)', line)[-1]\n",
    "\n",
    "        # Store the extracted values in the dictionary\n",
    "        line_numbers_dict[line_number] = (character_id, l)\n",
    "\n",
    "    # Print the first 10 entries of the dictionary (for verification purposes)\n",
    "    print(dict(itertools.islice(line_numbers_dict.items(), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Collect movie conversation lists\n",
    "    with open(cwd + \"/movie_lines/movie_conversations.txt\", \"r\", encoding=movie_conversations_encoding) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split the content into individual lines\n",
    "    lines = content.split(\"\\n\")\n",
    "\n",
    "    # Print the first 5 lines to verify content\n",
    "    print(lines[:5])\n",
    "\n",
    "    # Remove the last element of lines because it's an empty string\n",
    "    lines = lines[:-1]\n",
    "\n",
    "    # Print the length of the lines to verify correctness\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Initialize containers for values to put in the dataframe\n",
    "    speaker1_ids = []\n",
    "    speaker2_ids = []\n",
    "    conversation_lines = []\n",
    "\n",
    "    # Iterate through each line in the lines list\n",
    "    for line in lines:\n",
    "        # Split the line on whitespace\n",
    "        split = line.split(\" \")\n",
    "        \n",
    "        # Extract speaker IDs\n",
    "        speaker1_ids.append(split[0])\n",
    "        speaker2_ids.append(split[2])\n",
    "\n",
    "        # Extract the text after the last \"+\" character using regular expression\n",
    "        # This is a way to split the line on the last \"+\" in case there are other \"+\" characters in the text\n",
    "        l = re.split(r'\\+\\s+(?=[^+]*$)', line)[-1]\n",
    "\n",
    "        # Evaluate the literal expression to convert it into a Python object\n",
    "        l = ast.literal_eval(l)\n",
    "        \n",
    "        # Append the conversation lines to the list\n",
    "        conversation_lines.append(l)\n",
    "\n",
    "    # Create a dataframe from the extracted values\n",
    "    movie_conversations = pd.DataFrame(list(zip(speaker1_ids, speaker2_ids, conversation_lines)),\n",
    "                                       columns=[\"speaker1_id\", \"speaker2_id\", \"conversation_lines\"])\n",
    "\n",
    "    # Display the first few rows of the dataframe\n",
    "    display(movie_conversations.head())\n",
    "\n",
    "    # Display information about the dataframe (e.g., data types, memory usage)\n",
    "    movie_conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for turning movie lines into multi-turn conversations for training\n",
    "def create_conversation_turns(row):\n",
    "    # Extract speaker1 ID and conversation lines from the row\n",
    "    speaker1 = row[\"speaker1_id\"]\n",
    "    conversation_list = row[\"conversation_lines\"]\n",
    "    \n",
    "    # Initialize an empty string to store the conversation\n",
    "    convo = \"\"\n",
    "    \n",
    "    # For each line in the conversation, add it to the conversation string with a role label\n",
    "    for line_id in conversation_list:\n",
    "        # Retrieve the movie line details from the preprocessed dictionary\n",
    "        movie_line = line_numbers_dict[line_id]\n",
    "        text = movie_line[1]\n",
    "\n",
    "        # Determine the role label based on the speaker ID\n",
    "        if movie_line[0] == speaker1:\n",
    "            convo += f\"<USER>: {text} \\n\"\n",
    "        else:\n",
    "            convo += f\"<AGENT>: {text} \\n\"\n",
    "    \n",
    "    return convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Try the create_conversation_turns function on a sample for testing\n",
    "    sample = movie_conversations.copy().iloc[:2]\n",
    "    sample[\"conversation\"] = sample.apply(create_conversation_turns, axis=1)\n",
    "    display(sample)\n",
    "\n",
    "    # Apply create_conversation_turns on every row of the dataframe\n",
    "    movie_conversations[\"conversation\"] = movie_conversations.apply(create_conversation_turns, axis=1)\n",
    "\n",
    "    # Drop speaker ID columns because they aren't needed anymore\n",
    "    movie_conversations.drop(columns=[\"speaker1_id\", \"speaker2_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Display the first few rows of the 'movie_conversations' DataFrame\n",
    "    display(movie_conversations.head())\n",
    "\n",
    "    # Print the conversation text of the first row for verification\n",
    "    print(movie_conversations[\"conversation\"].head(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Initialize GPT-2 tokenizer from Hugging Face's Transformers library\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "    # Calculate the token count for each conversation to identify ones that are too long\n",
    "    movie_conversations[\"token_count\"] = movie_conversations[\"conversation\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "    # Display the first few rows of the 'movie_conversations' DataFrame with the new 'token_count' column\n",
    "    movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:    \n",
    "    # Get the conversation entries that are over the token limit\n",
    "    over_token_limit = movie_conversations[movie_conversations[\"token_count\"] > MAX_TOKEN_LENGTH].index\n",
    "    print(over_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # NOTE: Be careful not to run this more than once without resetting the dataframe\n",
    "\n",
    "    # Initialize a list to store new entries after splitting conversations\n",
    "    new_entries = []\n",
    "\n",
    "    # For each conversation that is too long, split it in half\n",
    "    for idx in over_token_limit:\n",
    "        lines_to_split = movie_conversations.iloc[idx][\"conversation_lines\"]\n",
    "        split_idx = (len(lines_to_split)//2)\n",
    "        first_half = lines_to_split[:split_idx]\n",
    "        second_half = lines_to_split[split_idx:]\n",
    "\n",
    "        first_convo = \"\"\n",
    "        second_convo = \"\"\n",
    "\n",
    "        # For each line in the first half, add it to the conversation\n",
    "        for i, line_id in enumerate(first_half):\n",
    "            movie_line = line_numbers_dict[line_id]\n",
    "            text = movie_line[1]\n",
    "\n",
    "            # Alternate between USER and AGENT roles\n",
    "            if i % 2 == 0:\n",
    "                first_convo += f\"<USER>: {text} \\n\"\n",
    "            else:\n",
    "                first_convo += f\"<AGENT>: {text} \\n\"\n",
    "\n",
    "        # Append the new entry for the first half to the list\n",
    "        new_entries.append({\"conversation_lines\": first_half, \"conversation\": first_convo})\n",
    "\n",
    "        # For each line in the second half, add it to the conversation\n",
    "        for i, line_id in enumerate(second_half):\n",
    "            movie_line = line_numbers_dict[line_id]\n",
    "            text = movie_line[1]\n",
    "\n",
    "            # Alternate between USER and AGENT roles\n",
    "            if i % 2 == 0:\n",
    "                second_convo += f\"<USER>: {text} \\n\"\n",
    "            else:\n",
    "                second_convo += f\"<AGENT>: {text} \\n\"\n",
    "\n",
    "        # Append the new entry for the second half to the list\n",
    "        new_entries.append({\"conversation_lines\": second_half, \"conversation\": second_convo})\n",
    "\n",
    "    # Add the new entries from splitting and drop the originals\n",
    "    movie_conversations = pd.concat([movie_conversations, pd.DataFrame(new_entries)], axis=0, ignore_index=True)\n",
    "    movie_conversations.drop(index=over_token_limit, inplace=True)\n",
    "\n",
    "    # Reset the index of the dataframe\n",
    "    movie_conversations.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Display information about the updated 'movie_conversations' DataFrame\n",
    "    movie_conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Check that no conversations are over the token limit\n",
    "    display(movie_conversations[movie_conversations[\"token_count\"] > MAX_TOKEN_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe from file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker1_id</th>\n",
       "      <th>speaker2_id</th>\n",
       "      <th>conversation_lines</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>[L194, L195, L196, L197]</td>\n",
       "      <td>&lt;USER&gt;: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n&lt;AGENT&gt;: Well, I thought we'd start with pronunciation, if that's okay with you. \\n&lt;USER&gt;: Not the hacking and gagging and spitting part.  Please. \\n&lt;AGENT&gt;: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>[L198, L199]</td>\n",
       "      <td>&lt;USER&gt;: You're asking me out.  That's so cute. What's your name again? \\n&lt;AGENT&gt;: Forget it. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>[L200, L201, L202, L203]</td>\n",
       "      <td>&lt;USER&gt;: No, no, it's my fault -- we didn't have a proper introduction --- \\n&lt;AGENT&gt;: Cameron. \\n&lt;USER&gt;: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does. \\n&lt;AGENT&gt;: Seems like she could get a date easy enough... \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>[L204, L205, L206]</td>\n",
       "      <td>&lt;AGENT&gt;: Why? \\n&lt;USER&gt;: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something. \\n&lt;AGENT&gt;: That's a shame. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>[L207, L208]</td>\n",
       "      <td>&lt;USER&gt;: Gosh, if only we could find Kat a boyfriend... \\n&lt;AGENT&gt;: Let me see what I can do. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker1_id speaker2_id        conversation_lines  \\\n",
       "0          u0          u2  [L194, L195, L196, L197]   \n",
       "1          u0          u2              [L198, L199]   \n",
       "2          u0          u2  [L200, L201, L202, L203]   \n",
       "3          u0          u2        [L204, L205, L206]   \n",
       "4          u0          u2              [L207, L208]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                 conversation  \n",
       "0  <USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \\n<USER>: Not the hacking and gagging and spitting part.  Please. \\n<AGENT>: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \\n  \n",
       "1                                                                                                                                                                                                                                                                                             <USER>: You're asking me out.  That's so cute. What's your name again? \\n<AGENT>: Forget it. \\n  \n",
       "2                                                                                          <USER>: No, no, it's my fault -- we didn't have a proper introduction --- \\n<AGENT>: Cameron. \\n<USER>: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does. \\n<AGENT>: Seems like she could get a date easy enough... \\n  \n",
       "3                                                                                                                                                                                                 <AGENT>: Why? \\n<USER>: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something. \\n<AGENT>: That's a shame. \\n  \n",
       "4                                                                                                                                                                                                                                                                                              <USER>: Gosh, if only we could find Kat a boyfriend... \\n<AGENT>: Let me see what I can do. \\n  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \n",
      "<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \n",
      "<USER>: Not the hacking and gagging and spitting part.  Please. \n",
      "<AGENT>: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset has not been generated\n",
    "if not dataset_generated:\n",
    "    # Save the 'movie_conversations' DataFrame to a file using pickle\n",
    "    with open(DATASET_FILENAME, \"wb\") as f:\n",
    "        print(\"Writing dataframe to file\")\n",
    "        pickle.dump(movie_conversations, f)\n",
    "else:\n",
    "    # Load the 'movie_conversations' DataFrame from the saved file\n",
    "    with open(DATASET_FILENAME, \"rb\") as f:\n",
    "        print(\"Loading dataframe from file\")\n",
    "        movie_conversations = pickle.load(f)\n",
    "\n",
    "# Display the first few rows of the 'movie_conversations' DataFrame\n",
    "display(movie_conversations.head())\n",
    "\n",
    "# Print the conversation text of the first row for verification\n",
    "print(movie_conversations[\"conversation\"].head(1).values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------- Fine tune + Train -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./myenv/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: transformers in ./myenv/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in ./myenv/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./myenv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./myenv/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in ./myenv/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./myenv/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.10/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.10/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./myenv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pkl to txt\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load data from pickle file\n",
    "# with open('movie_conversations.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# # Assuming your DataFrame has a 'text_column' containing text data\n",
    "# if isinstance(data, pd.DataFrame) and 'conversation' in data.columns:\n",
    "#     text_content = '\\n'.join(data['conversation'].astype(str))\n",
    "# else:\n",
    "#     raise ValueError(\"Unexpected data format in the pickle file\")\n",
    "\n",
    "# # Save the text content to a text file\n",
    "# with open('movie_convo.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./myenv/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.10/site-packages (from accelerate) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./myenv/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in ./myenv/lib/python3.10/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in ./myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gpt2(model_name, train_file, output_dir):\n",
    "    # Load GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load training dataset\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_file,\n",
    "        block_size=128)\n",
    "\n",
    "    # Create data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/myenv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "  4%|‚ñç         | 500/12164 [12:15<5:31:31,  1.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6, 'learning_rate': 4.7944755014797766e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 1000/12164 [21:37<3:16:28,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5745, 'learning_rate': 4.588951002959553e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 1500/12164 [30:46<3:13:56,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5416, 'learning_rate': 4.3834265044393294e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñã        | 2000/12164 [39:52<3:05:44,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5363, 'learning_rate': 4.177902005919106e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 2500/12164 [49:19<2:54:11,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5262, 'learning_rate': 3.972377507398882e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñç       | 3000/12164 [1:32:22<4:49:47,  1.90s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5462, 'learning_rate': 3.7668530088786585e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 3500/12164 [2:19:21<12:38:02,  5.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5143, 'learning_rate': 3.561328510358435e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 4000/12164 [2:56:49<8:42:51,  3.84s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5121, 'learning_rate': 3.355804011838211e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 4500/12164 [3:15:44<2:19:06,  1.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5256, 'learning_rate': 3.150279513317988e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 5000/12164 [3:35:05<2:10:40,  1.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5009, 'learning_rate': 2.9447550147977644e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 5500/12164 [4:09:54<1:59:47,  1.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4945, 'learning_rate': 2.7392305162775405e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 6000/12164 [4:27:24<1:56:38,  1.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4885, 'learning_rate': 2.533706017757317e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 6500/12164 [4:38:19<1:57:37,  1.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5084, 'learning_rate': 2.3281815192370932e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7000/12164 [4:47:53<1:34:55,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4806, 'learning_rate': 2.1226570207168696e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 7500/12164 [5:22:08<1:23:37,  1.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4713, 'learning_rate': 1.917132522196646e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 8000/12164 [6:14:58<1:42:04,  1.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.464, 'learning_rate': 1.7116080236764224e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 8500/12164 [6:24:14<1:08:38,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4803, 'learning_rate': 1.5060835251561986e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 9000/12164 [6:34:42<59:22,  1.13s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4743, 'learning_rate': 1.3005590266359752e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 9500/12164 [6:46:37<54:54,  1.24s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4649, 'learning_rate': 1.0950345281157515e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 10000/12164 [7:12:56<7:47:29, 12.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4507, 'learning_rate': 8.895100295955277e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 10500/12164 [7:26:08<30:41,  1.11s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4498, 'learning_rate': 6.839855310753042e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 11000/12164 [7:35:29<22:10,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4637, 'learning_rate': 4.784610325550805e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 11500/12164 [7:44:50<12:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4649, 'learning_rate': 2.7293653403485694e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 12000/12164 [7:54:04<03:03,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4701, 'learning_rate': 6.741203551463335e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12164/12164 [7:57:07<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28627.4092, 'train_samples_per_second': 1.7, 'train_steps_per_second': 0.425, 'train_loss': 2.499623904217234, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "fine_tune_gpt2(\n",
    "    \"gpt2-medium\",\n",
    "    \"/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/movie_convo.txt\",\n",
    "    \"/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt/tokenizer_config.json',\n",
       " '/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt/special_tokens_map.json',\n",
       " '/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt/vocab.json',\n",
       " '/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt/merges.txt',\n",
       " '/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt/added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.save_pretrained(\"/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt\")\n",
    "tokenizer.save_pretrained(\"/Users/maimuna/Desktop/AI Masters @ USD/AAI-520-02 - FALL23-Natural Language Processing /generative-chat-bot/NEWmovie.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how was your day?\n",
      "\n",
      "I was fine. I was just tired.\n",
      "\n",
      "What was your favorite part of the day?\n",
      "\n",
      "I was just enjoying the day.\n",
      "\n",
      "What was your worst part of the day?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def generate_response(prompt, model, tokenizer, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hello, how was your day?\"\n",
    "response = generate_response(prompt, model, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
