{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import chardet\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'PyTorch version: {torch.__version__}')\n",
    "# print('*'*10)\n",
    "# print(f'_CUDA version: ')\n",
    "# !nvcc --version\n",
    "# print('*'*10)\n",
    "# print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "# print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "# print(f'Device Name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILENAME = \"movie_conversations.pkl\"\n",
    "MAX_TOKEN_LENGTH = 1024\n",
    "dataset_generated = False\n",
    "\n",
    "if os.path.exists(DATASET_FILENAME):\n",
    "    dataset_generated = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Detect encoding of the movie lines file\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # with open(cwd + \"/movie_lines/movie_lines.txt\", \"rb\") as f:\n",
    "    #     result = chardet.detect(f.read())\n",
    "    #     movie_lines_encoding = result[\"encoding\"]\n",
    "\n",
    "    # with open(cwd + \"/movie_lines/movie_conversations.txt\", \"rb\") as f:\n",
    "    #     result = chardet.detect(f.read())\n",
    "    #     movie_conversations_encoding = result[\"encoding\"]\n",
    "\n",
    "    # print(movie_lines_encoding)\n",
    "    # print(movie_conversations_encoding)\n",
    "    movie_lines_encoding = \"Windows-1252\"\n",
    "    movie_conversations_encoding = \"ascii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Collect individual movie lines\n",
    "    with open(cwd + \"/movie_lines/movie_lines.txt\", \"r\", encoding=movie_lines_encoding) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    lines = content.split(\"\\n\")\n",
    "\n",
    "    # Print first 5 lines and verify length is correct\n",
    "    print(lines[:5])\n",
    "\n",
    "    # Remove last element of lines because its an empty string\n",
    "    lines = lines[:-1]\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Initialize containers for values to put in dataframe\n",
    "    line_numbers_dict = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # Split on whitespace\n",
    "        split = line.split(\" \")\n",
    "        line_number = split[0]\n",
    "        character_id = split[2]\n",
    "\n",
    "        # Extract the text after the last \"+\" character\n",
    "        l = re.split(r'\\+\\s+(?=[^+]*$)', line)[-1]\n",
    "        line_numbers_dict[line_number] = (character_id, l)\n",
    "\n",
    "\n",
    "    # Create dataframe from extracted values\n",
    "    print(dict(itertools.islice(line_numbers_dict.items(), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Collect movie conversation lists\n",
    "    with open(cwd + \"/movie_lines/movie_conversations.txt\", \"r\", encoding=movie_conversations_encoding) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    lines = content.split(\"\\n\")\n",
    "\n",
    "    # Print first 5 lines and verify length is correct\n",
    "    print(lines[:5])\n",
    "\n",
    "    # Remove last element of lines because its an empty string\n",
    "    lines = lines[:-1]\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Initialize containers for values to put in dataframe\n",
    "    speaker1_ids = []\n",
    "    speaker2_ids = []\n",
    "    conversation_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Split on whitespace\n",
    "        split = line.split(\" \")\n",
    "        speaker1_ids.append(split[0])\n",
    "        speaker2_ids.append(split[2])\n",
    "\n",
    "        # Extract the text after the last \"+\" character\n",
    "        l = re.split(r'\\+\\s+(?=[^+]*$)', line)[-1]\n",
    "        l = ast.literal_eval(l)\n",
    "        conversation_lines.append(l)\n",
    "\n",
    "\n",
    "    # Create dataframe from extracted values\n",
    "    movie_conversations = pd.DataFrame(list(zip(speaker1_ids, speaker2_ids, conversation_lines)), columns=[\"speaker1_id\", \"speaker2_id\", \"conversation_lines\"])\n",
    "    display(movie_conversations.head())\n",
    "    movie_conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for turning movie lines into multi-turn conversations for training\n",
    "def create_conversation_turns(row):\n",
    "    speaker1 = row[\"speaker1_id\"]\n",
    "    conversation_list = row[\"conversation_lines\"]\n",
    "    convo = \"\"\n",
    "    # For each line, add it to the conversation with a role label\n",
    "    for i, line_id in enumerate(conversation_list):\n",
    "        movie_line = line_numbers_dict[line_id]\n",
    "        text = movie_line[1]\n",
    "\n",
    "        # Ensure the conversations end with agent response\n",
    "        if i == len(conversation_list) - 1 and i % 2 == 0:\n",
    "            continue\n",
    "\n",
    "        if i%2 == 0:\n",
    "            convo += f\"<USER>: {text} \\n\"\n",
    "        else:\n",
    "            convo += f\"<AGENT>: {text} \\n\"\n",
    "\n",
    "        # if movie_line[0] == speaker1:\n",
    "        #     convo += f\"<USER>: {text} \\n\"\n",
    "        # else:\n",
    "        #     convo += f\"<AGENT>: {text} \\n\"\n",
    "        \n",
    "    return convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Try it on a sample for testing\n",
    "    sample = movie_conversations.copy().iloc[:2]\n",
    "    sample[\"conversation\"] = sample.apply(create_conversation_turns, axis=1)\n",
    "    display(sample)\n",
    "\n",
    "    # Call create_conversation_turns on every row of the dataframe\n",
    "    movie_conversations[\"conversation\"] = movie_conversations.apply(create_conversation_turns, axis=1)\n",
    "\n",
    "    # Drop speaker id columns because they aren't needed anymore\n",
    "    movie_conversations.drop(columns=[\"speaker1_id\", \"speaker2_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    display(movie_conversations.head())\n",
    "    print(movie_conversations[\"conversation\"].head(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Get the token count for each conversation so we can split the ones that are too long\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    movie_conversations[\"token_count\"] = movie_conversations[\"conversation\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:    \n",
    "    # Get the conversation entries that are over the token limit\n",
    "    over_token_limit = movie_conversations[movie_conversations[\"token_count\"] > MAX_TOKEN_LENGTH].index\n",
    "    print(over_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # NOTE: Be careful not to run this more than once without reseting the dataframe\n",
    "    new_entries = []\n",
    "\n",
    "    # For each conversation that is too long, split it in half\n",
    "    for idx in over_token_limit:\n",
    "        lines_to_split = movie_conversations.iloc[idx][\"conversation_lines\"]\n",
    "        split_idx = (len(lines_to_split)//2)\n",
    "        first_half = lines_to_split[:split_idx]\n",
    "        second_half = lines_to_split[split_idx:]\n",
    "\n",
    "        first_convo = \"\"\n",
    "        second_convo = \"\"\n",
    "        # For each line, add it to the conversation\n",
    "        for i, line_id in enumerate(first_half):\n",
    "            movie_line = line_numbers_dict[line_id]\n",
    "            text = movie_line[1]\n",
    "\n",
    "            # Ensure the conversations end with agent response\n",
    "            if i == len(first_half) - 1 and i % 2 == 0:\n",
    "                continue\n",
    "\n",
    "            if i%2 == 0:\n",
    "                first_convo += f\"<USER>: {text} \\n\"\n",
    "            else:\n",
    "                first_convo += f\"<AGENT>: {text} \\n\"\n",
    "\n",
    "        new_entries.append({\"conversation_lines\": first_half, \"conversation\": first_convo})\n",
    "\n",
    "        for i, line_id in enumerate(second_half):\n",
    "            movie_line = line_numbers_dict[line_id]\n",
    "            text = movie_line[1]\n",
    "\n",
    "            # Ensure the conversations end with agent response\n",
    "            if i == len(second_half) - 1 and i % 2 == 0:\n",
    "                continue\n",
    "\n",
    "            if i%2 == 0:\n",
    "                second_convo += f\"<USER>: {text} \\n\"\n",
    "            else:\n",
    "                second_convo += f\"<AGENT>: {text} \\n\"\n",
    "\n",
    "        new_entries.append({\"conversation_lines\": second_half, \"conversation\": second_convo})\n",
    "\n",
    "    # Add the new entries from splitting and drop the originals\n",
    "    movie_conversations = pd.concat([movie_conversations, pd.DataFrame(new_entries)], axis=0, ignore_index=True)\n",
    "    movie_conversations.drop(index=over_token_limit, inplace=True)\n",
    "\n",
    "    movie_conversations.reset_index(inplace=True, drop=True)\n",
    "    movie_conversations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_generated:\n",
    "    # Check that no conversations are over the token limit\n",
    "    display(movie_conversations[movie_conversations[\"token_count\"] > MAX_TOKEN_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop line numbers and token count because they aren't needed anymore\n",
    "if not dataset_generated:\n",
    "    movie_conversations.drop(columns=[\"conversation_lines\", \"token_count\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe from file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;USER&gt;: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n&lt;AGENT&gt;: Well, I thought we'd start with pronunciation, if that's okay with you. \\n&lt;USER&gt;: Not the hacking and gagging and spitting part.  Please. \\n&lt;AGENT&gt;: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \\n</td>\n",
       "      <td>&lt;USER&gt;: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n&lt;AGENT&gt;: Well, I thought we'd start with pronunciation, if that's okay with you. \\n&lt;USER&gt;: Not the hacking and gagging and spitting part.  Please.&lt;AGENT&gt;:</td>\n",
       "      <td>Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;USER&gt;: You're asking me out.  That's so cute. What's your name again? \\n&lt;AGENT&gt;: Forget it. \\n</td>\n",
       "      <td>&lt;USER&gt;: You're asking me out.  That's so cute. What's your name again?&lt;AGENT&gt;:</td>\n",
       "      <td>Forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;USER&gt;: No, no, it's my fault -- we didn't have a proper introduction --- \\n&lt;AGENT&gt;: Cameron. \\n&lt;USER&gt;: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does. \\n&lt;AGENT&gt;: Seems like she could get a date easy enough... \\n</td>\n",
       "      <td>&lt;USER&gt;: No, no, it's my fault -- we didn't have a proper introduction --- \\n&lt;AGENT&gt;: Cameron. \\n&lt;USER&gt;: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.&lt;AGENT&gt;:</td>\n",
       "      <td>Seems like she could get a date easy enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;USER&gt;: Why? \\n&lt;AGENT&gt;: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something. \\n</td>\n",
       "      <td>&lt;USER&gt;: Why?&lt;AGENT&gt;:</td>\n",
       "      <td>Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;USER&gt;: Gosh, if only we could find Kat a boyfriend... \\n&lt;AGENT&gt;: Let me see what I can do. \\n</td>\n",
       "      <td>&lt;USER&gt;: Gosh, if only we could find Kat a boyfriend...&lt;AGENT&gt;:</td>\n",
       "      <td>Let me see what I can do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                 conversation  \\\n",
       "0  <USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \\n<USER>: Not the hacking and gagging and spitting part.  Please. \\n<AGENT>: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \\n   \n",
       "1                                                                                                                                                                                                                                                                                             <USER>: You're asking me out.  That's so cute. What's your name again? \\n<AGENT>: Forget it. \\n   \n",
       "2                                                                                          <USER>: No, no, it's my fault -- we didn't have a proper introduction --- \\n<AGENT>: Cameron. \\n<USER>: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does. \\n<AGENT>: Seems like she could get a date easy enough... \\n   \n",
       "3                                                                                                                                                                                                                            <USER>: Why? \\n<AGENT>: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something. \\n   \n",
       "4                                                                                                                                                                                                                                                                                              <USER>: Gosh, if only we could find Kat a boyfriend... \\n<AGENT>: Let me see what I can do. \\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  <USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \\n<USER>: Not the hacking and gagging and spitting part.  Please.<AGENT>:   \n",
       "1                                                                                                                                                                                                                              <USER>: You're asking me out.  That's so cute. What's your name again?<AGENT>:   \n",
       "2                                                               <USER>: No, no, it's my fault -- we didn't have a proper introduction --- \\n<AGENT>: Cameron. \\n<USER>: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.<AGENT>:   \n",
       "3                                                                                                                                                                                                                                                                                        <USER>: Why?<AGENT>:   \n",
       "4                                                                                                                                                                                                                                              <USER>: Gosh, if only we could find Kat a boyfriend...<AGENT>:   \n",
       "\n",
       "                                                                                                                                response  \n",
       "0                                                              Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?  \n",
       "1                                                                                                                             Forget it.  \n",
       "2                                                                                         Seems like she could get a date easy enough...  \n",
       "3  Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.  \n",
       "4                                                                                                              Let me see what I can do.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \n",
      "<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \n",
      "<USER>: Not the hacking and gagging and spitting part.  Please. \n",
      "<AGENT>: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Serialize dataset for faster loading\n",
    "if not dataset_generated:\n",
    "    with open(DATASET_FILENAME, \"wb\") as f:\n",
    "        print(\"Writing dataframe to file\")\n",
    "        pickle.dump(movie_conversations, f)\n",
    "else:\n",
    "    with open(DATASET_FILENAME, \"rb\") as f:\n",
    "        print(\"Loading dataframe from file\")\n",
    "        movie_conversations = pickle.load(f)\n",
    "\n",
    "display(movie_conversations.head())\n",
    "print(movie_conversations[\"conversation\"].head(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \n",
      "<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \n",
      "<USER>: Not the hacking and gagging and spitting part.  Please. \n",
      "<AGENT>: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \n",
      "\n",
      "Input:\n",
      " <USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \n",
      "<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \n",
      "<USER>: Not the hacking and gagging and spitting part.  Please.<AGENT>:\n",
      "Response:\n",
      " Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "<USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \n",
      "\n",
      " Well, I thought we'd start with pronunciation, if that's okay with you. \n",
      "<USER>: Not the hacking and gagging and spitting part.  Please. \n",
      "\n",
      " Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test splitting on last agent line\n",
    "test_string = movie_conversations[\"conversation\"].head(1).values[0]\n",
    "print(test_string)\n",
    "splits = test_string.split('<AGENT>:')\n",
    "response = splits[-1].strip()\n",
    "input = '<AGENT>:'.join(splits[:-1]).strip() + \"<AGENT>:\"\n",
    "print(\"Input:\\n\", input)\n",
    "print(\"Response:\\n\", response)\n",
    "for split in splits:\n",
    "    print(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;USER&gt;: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n&lt;AGENT&gt;: Well, I thought we'd start with pronunciation, if that's okay with you. \\n&lt;USER&gt;: Not the hacking and gagging and spitting part.  Please. \\n&lt;AGENT&gt;: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \\n</td>\n",
       "      <td>&lt;USER&gt;: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n&lt;AGENT&gt;: Well, I thought we'd start with pronunciation, if that's okay with you. \\n&lt;USER&gt;: Not the hacking and gagging and spitting part.  Please.&lt;AGENT&gt;:</td>\n",
       "      <td>Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;USER&gt;: You're asking me out.  That's so cute. What's your name again? \\n&lt;AGENT&gt;: Forget it. \\n</td>\n",
       "      <td>&lt;USER&gt;: You're asking me out.  That's so cute. What's your name again?&lt;AGENT&gt;:</td>\n",
       "      <td>Forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;USER&gt;: No, no, it's my fault -- we didn't have a proper introduction --- \\n&lt;AGENT&gt;: Cameron. \\n&lt;USER&gt;: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does. \\n&lt;AGENT&gt;: Seems like she could get a date easy enough... \\n</td>\n",
       "      <td>&lt;USER&gt;: No, no, it's my fault -- we didn't have a proper introduction --- \\n&lt;AGENT&gt;: Cameron. \\n&lt;USER&gt;: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.&lt;AGENT&gt;:</td>\n",
       "      <td>Seems like she could get a date easy enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;USER&gt;: Why? \\n&lt;AGENT&gt;: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something. \\n</td>\n",
       "      <td>&lt;USER&gt;: Why?&lt;AGENT&gt;:</td>\n",
       "      <td>Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;USER&gt;: Gosh, if only we could find Kat a boyfriend... \\n&lt;AGENT&gt;: Let me see what I can do. \\n</td>\n",
       "      <td>&lt;USER&gt;: Gosh, if only we could find Kat a boyfriend...&lt;AGENT&gt;:</td>\n",
       "      <td>Let me see what I can do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                 conversation  \\\n",
       "0  <USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \\n<USER>: Not the hacking and gagging and spitting part.  Please. \\n<AGENT>: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night? \\n   \n",
       "1                                                                                                                                                                                                                                                                                             <USER>: You're asking me out.  That's so cute. What's your name again? \\n<AGENT>: Forget it. \\n   \n",
       "2                                                                                          <USER>: No, no, it's my fault -- we didn't have a proper introduction --- \\n<AGENT>: Cameron. \\n<USER>: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does. \\n<AGENT>: Seems like she could get a date easy enough... \\n   \n",
       "3                                                                                                                                                                                                                            <USER>: Why? \\n<AGENT>: Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something. \\n   \n",
       "4                                                                                                                                                                                                                                                                                              <USER>: Gosh, if only we could find Kat a boyfriend... \\n<AGENT>: Let me see what I can do. \\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      context  \\\n",
       "0  <USER>: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \\n<AGENT>: Well, I thought we'd start with pronunciation, if that's okay with you. \\n<USER>: Not the hacking and gagging and spitting part.  Please.<AGENT>:   \n",
       "1                                                                                                                                                                                                                              <USER>: You're asking me out.  That's so cute. What's your name again?<AGENT>:   \n",
       "2                                                               <USER>: No, no, it's my fault -- we didn't have a proper introduction --- \\n<AGENT>: Cameron. \\n<USER>: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.<AGENT>:   \n",
       "3                                                                                                                                                                                                                                                                                        <USER>: Why?<AGENT>:   \n",
       "4                                                                                                                                                                                                                                              <USER>: Gosh, if only we could find Kat a boyfriend...<AGENT>:   \n",
       "\n",
       "                                                                                                                                response  \n",
       "0                                                              Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?  \n",
       "1                                                                                                                             Forget it.  \n",
       "2                                                                                         Seems like she could get a date easy enough...  \n",
       "3  Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.  \n",
       "4                                                                                                              Let me see what I can do.  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the test and evaluation sets, cut off the last agent response to create expected output to compare against for eval\n",
    "def create_response(text):\n",
    "    splits = text.split('<AGENT>:')\n",
    "    input = '<AGENT>:'.join(splits[:-1]).strip() + \"<AGENT>:\"\n",
    "    response = splits[-1].strip()\n",
    "\n",
    "    return pd.Series([input, response])\n",
    "\n",
    "movie_conversations[[\"context\", \"response\"]] = movie_conversations[\"conversation\"].apply(create_response)\n",
    "movie_conversations.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49864 entries, 30425 to 15795\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   conversation  49864 non-null  object\n",
      " 1   context       49864 non-null  object\n",
      " 2   response      49864 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16621 entries, 72890 to 1330\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   conversation  16621 non-null  object\n",
      " 1   context       16621 non-null  object\n",
      " 2   response      16621 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 519.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16622 entries, 43213 to 20794\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   conversation  16622 non-null  object\n",
      " 1   context       16622 non-null  object\n",
      " 2   response      16622 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 519.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into test, evaluation, and training sets\n",
    "train_data, temp_data = train_test_split(movie_conversations, train_size=0.6, random_state=42)\n",
    "print(train_data.info())\n",
    "\n",
    "eval_data, test_data = train_test_split(temp_data, train_size=0.5, random_state=42)\n",
    "print(eval_data.info())\n",
    "print(test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation', 'context', 'response'],\n",
       "    num_rows: 49864\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataframes to datasets\n",
    "train_dataset = Dataset.from_pandas(train_data, preserve_index=False)\n",
    "eval_dataset = Dataset.from_pandas(eval_data, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_data, preserve_index=False)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\", pad_token='<|pad|>')\n",
    "\n",
    "# Tokenize the features\n",
    "# def tokenize_function(dataset):\n",
    "#     conversation = tokenizer(dataset[\"conversation\"], padding=True, return_tensors=\"pt\")\n",
    "#     context = tokenizer(dataset[\"context\"], padding=True, return_tensors=\"pt\")\n",
    "#     response = tokenizer(dataset[\"response\"], padding=True, return_tensors=\"pt\")\n",
    "#     return {\n",
    "#         \"conversation_input_ids\": conversation[\"input_ids\"], \n",
    "#         \"conversation_attention_mask\": conversation[\"attention_mask\"],\n",
    "#         \"context_input_ids\": context[\"input_ids\"], \n",
    "#         \"context_attention_mask\": context[\"attention_mask\"],\n",
    "#         \"response_input_ids\": response[\"input_ids\"], \n",
    "#         \"response_attention_mask\": response[\"attention_mask\"]\n",
    "#     }\n",
    "\n",
    "# train_data_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "# eval_data_tokenized = eval_dataset.map(tokenize_function, batched=True)\n",
    "# test_data_tokenized = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f292371ae3354623b0a9f4e9d611f13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd34f7349367447e896edfa3737fb1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16621 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    input_data = tokenizer(dataset[\"context\"], return_tensors=\"pt\", padding=\"max_length\", max_length=1024)\n",
    "    labels_data = tokenizer(dataset[\"response\"], return_tensors=\"pt\", padding=\"max_length\", max_length=1024)\n",
    "    return {\n",
    "        \"input_ids\": input_data[\"input_ids\"], \n",
    "        \"attention_mask\": input_data[\"attention_mask\"],\n",
    "        \"labels\": labels_data[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "train_data_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_data_tokenized = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation', 'context', 'response', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 49864\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<USER>: Thelma. \n",
      "<AGENT>: Yeah. \n",
      "<USER>: I want you to call Darryl. \n",
      "<AGENT>: What for? \n",
      "<USER>: To find out if he knows anything. If you think he does, you gotta hang up because it means the police have told him and the phone is probably tapped. \n",
      "<AGENT>: Jeez, Louise, tapped the phone?  You think so? \n",
      "<USER>: Oh, come on!  Murder one and armed robbery, Thelma! \n",
      "<AGENT>: Murder one!  God, Louise, can't we even say it was self-defense? \n",
      "<USER>: But it wasn't!  We got away!  We were walkin' away! \n",
      "<AGENT>: They don't know that!  It was just you and me there.  I'll say he raped me and you had to shoot him!  I mean, it's almost the truth! \n",
      "<USER>: It won't work.<AGENT>:\n",
      "[27, 29904, 31175, 383, 75, 2611, 13, 220, 198, 27, 4760, 3525, 31175, 9425, 13, 220, 198, 27, 29904, 31175, 314, 765, 345, 284, 869, 360, 6532, 75, 13, 220, 198, 27, 4760, 3525, 31175, 1867, 329, 30, 220, 198, 27, 29904, 31175, 1675, 1064, 503, 611, 339, 4206, 1997, 13, 1002, 345, 892, 339, 857, 11, 345, 17753, 8181, 510, 780, 340, 1724, 262, 1644, 423, 1297, 683, 290, 262, 3072, 318, 2192, 22499, 13, 220, 198, 27, 4760, 3525, 31175, 449, 33105, 11, 31774, 11, 22499, 262, 3072, 30, 220, 921, 892, 523, 30, 220, 198, 27, 29904, 31175, 3966, 11, 1282, 319, 0, 220, 23375, 530, 290, 6936, 18609, 11, 383, 75, 2611, 0, 220, 198, 27, 4760, 3525, 31175, 23375, 530, 0, 220, 1793, 11, 31774, 11, 460, 470, 356, 772, 910, 340, 373, 2116, 12, 19774, 30, 220, 198, 27, 29904, 31175, 887, 340, 2492, 470, 0, 220, 775, 1392, 1497, 0, 220, 775, 547, 2513, 259, 6, 1497, 0, 220, 198, 27, 4760, 3525, 31175, 1119, 836, 470, 760, 326, 0, 220, 632, 373, 655, 345, 290, 502, 612, 13, 220, 314, 1183, 910, 339, 16110, 502, 290, 345, 550, 284, 2686, 683, 0, 220, 314, 1612, 11, 340, 338, 2048, 262, 3872, 0, 220, 198, 27, 29904, 31175, 632, 1839, 470, 670, 29847, 4760, 3525, 31175, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sample = train_data_tokenized[0]\n",
    "print(sample[\"context\"])\n",
    "print(sample[\"input_ids\"])\n",
    "print(sample[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50259. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download base model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set model to run on CUDA\n",
    "#model = model.to(\"cpu\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/training_args.py:1257: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure training argumets for the trainer and method for evaluation\n",
    "checkpoint_dir_name = \"gpt2-trainer\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_dir_name, \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    resume_from_checkpoint=checkpoint_dir_name,\n",
    "    num_train_epochs=1,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# metric = load(\"perplexity\", module_type=\"metric\")\n",
    "# def compute_metrics(prediction):\n",
    "#     results = metric.compute(predictions=prediction, model_id='gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_data_tokenized.shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = eval_data_tokenized.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8269a56393407b8766bfe946e79ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb Cell 35\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run the trainer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39msmall_train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39msmall_eval_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juliet/Desktop/MSAAI/AAI-520/FinalProject/project.ipynb#Y131sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1557\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1558\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1559\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1560\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1561\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1837\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1838\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1841\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1843\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1844\u001b[0m ):\n\u001b[1;32m   1845\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2692\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2693\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2696\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/trainer.py:2718\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2717\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2718\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2719\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2721\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 354\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc(hidden_states)\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/MSAAI/AAI-520/FinalProject/.env/lib/python3.9/site-packages/transformers/pytorch_utils.py:106\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    105\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 106\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    107\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = pipeline('text-generation', model='gpt2-medium')\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"User: Hi, how are you? Agent: Ok, I guess it's time to show myself\"},\n",
       " {'generated_text': \"User: Hi, how are you? Agent: What's going on?! Player: Ok, I\"},\n",
       " {'generated_text': 'User: Hi, how are you? Agent: Hi! How would you like to play? What'},\n",
       " {'generated_text': \"User: Hi, how are you? Agent: I'm fine... Agent: What do you need\"},\n",
       " {'generated_text': \"User: Hi, how are you? Agent: Good, what happens is, I'm a person\"}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"User: Hi, how are you? Agent:\", max_length=20, num_return_sequences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
